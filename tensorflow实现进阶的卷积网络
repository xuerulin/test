#使用的数据集为CIFAR-10
#首先下载tensorflow models库
git clone https://github.com/tensorflow/models.git
cd models/tutorials/image/cifar10

import cifar10,cifar10_input
import tensorflow as tf
import numpy as np
import time

#接着定义batch_size、训练轮数max_steps,以及下载CIFAR-10数据的默认路径
max_steps = 3000
batch_size = 128
data_dir = '/tmp/cifar10_data/cifar-10-batches-bin'

#初始化weight的函数，依然使用tf.truncated_normal截断的正态分布来初始化权重，这里会给weight加一个L2的loss，相当于做了一个L2的正则化处理。
#一般来说，L1会制造稀疏的特征，大部分无用特征的权重会被置为0，而L2正则会让特征的权重不过大，使得特征的权重比较平均。
#使用w1控制L2 loss的大小，使用tf.nn.l2_loss函数计算weight的L2 loss
def variable_with_weight_loss(shape,stddev,w1):
  var = tf.Variable(tf.truncated_normal(shape,stddev=stddev))
  if w1 is not None:
    weight_loss = tf.multiply(tf.nn.l2_loss(var),w1,name='weight_loss')
    #使用add_to_collection把weight_loss统一存到一个collection
    tf.add_to_collection('losses',weight_loss)
  return var

#使用cifar10类下载数据集，并解压、展开到其默认位置
cifar10.maybe_download_and_extract()

images_train, labels_train = cifar10_input.distorted_inputs(data_dir=data_dir, batch_size=batch_size)
images_test, labels_test = cifar10_input.inputs(eval_data=True, data_dir=data_dir, batch_size=batch_size)

image_holder = tf.placeholder(tf.float32,[batch_size,24,24,3])
label_holder = tf.placeholder(tf.int32,[batch_size])

#接下来创建第一个卷积层
#第一个卷积层使用5x5的卷积核大小，3个颜色通道，64个卷积核，同时设置weight初始化函数的标准差为0.05
weight1 = variable_with_weight_loss(shape = [5,5,3,64], stddev = 5e-2, wl = 0.0)

#使用tf.nn.conv2d函数对输入数据image_holder进行卷积操作，这里的步长
kernel1 = tf.nn.conv2d(image_holder, weight1, [1,1,1,1], padding = 'SAME')
#该层的bias初始化全部为0，再将卷积的结果加上bias，然后使用relu激活函数进行非线性化
bias1 = tf.Variable(tf.constant(0.0, shape = [64]))
conv1 = tf.nn.relu(tf.nn.bias_add(kernel1,bias1))
#使用一个尺寸为3x3且步长为2x2的最大池化层处理数据
pool1 = tf.nn.max_pool(conv1, ksize=[1,3,3,1], strides=[1,2,2,1],padding='SAME')
#LRN对relu这种没有上限边界的激活函数会比较有用，因为它会从附近的多个卷积核的响应中挑选比较大的反馈
norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001/9.0, beta=0.75)

#创建第二个卷积层
#区别的地方：上一层的卷积核数量为64（即输出64个通道），所以本层卷积核尺寸的第三个维度即输入的通道数也需要调整为64；
weight2 = variable_with_weight_loss(shape = [5,5,64,64], stddev = 5e-2, wl = 0.0)
kernel2 = tf.nn.conv2d(norm1, weight2, [1,1,1,1], padding = 'SAME')
#bias初始化为0.1，同时调换了最大池化层和LRN层的顺序
bias2 = tf.Variable(tf.constant(0.1, shape=[64]))
conv2 = tf.nn.relu(tf.nn.bias_add(kernel2,bias2))
norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001/9.0, beta=0.75)
pool2 = tf.nn.max_pool(norm2, ksize=[1,3,3,1], strides=[1,2,2,1],padding='SAME')

#两个卷积层后，使用一个全连接层，首先需要将两个卷积层的输出结果全部flatten，使用tf.reshape函数将每个样本都变成一维向量
reshape = tf.reshape(pool2,[batch_size, -1])
#使用get_shape()函数，获取数据扁平化后的长度。
dim = reshape.get_shape()[1].value
#这里隐含节点数为384，由于希望全连接层不要过拟合，因此设定了一个非零的weight loss=0.004
weight3 = variable_with_weight_loss(shape = [dim,384], stddev = 0.04, wl = 0.004)
bias3 = tf.Variable(tf.constant(0.1, shape=[384]))
local3 = tf.nn.relu(tf.matmul(reshape,weight3)+bias3)

#接下来的全连接层同上，隐含节点数下降了一半，只有192个，其他的超参数不变
weight4 = variable_with_weight_loss(shape = [384,192], stddev = 0.04, wl = 0.004)
bias4 = tf.Variable(tf.constant(0.1, shape=[192]))
local4 = tf.nn.relu(tf.matmul(reshape,weight3)+bias3)

#接着是最后一层，正态分布的标准差设为上一个隐含层的节点数的倒数
weight5 = variable_with_weight_loss(shape = [192,10], stddev = 1/192.0, wl = 0.0)
bias5 = tf.Variable(tf.constant(0.0, shape=[10]))
logits = tf.add(tf.matmul(local4,weight5)+bias5)

#计算CNN的loss
def loss(logits, labels):
  labels = tf.cast(labels,tf.int64)
  cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels, name='cross_entropy_per_example')
  cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')
  tf.add_to_collection('losses',cross_entropy_mean)
  return tf.add_n(tf.get_collection('losses'),name='total_loss')
  
#接着将logits节点和label_placeholder传入loss函数获得最终的loss
loss = loss(logits,label_holder)
#优化器依然选择Adam Optimizer
train_op = tf.train.AdamOptimizer(1e-3).minimize(loss)
#使用tf.nn.in_top_k函数求输出结果中topk的准确率
top_k_op = tf.nn.in_top_k(logits, label_holder, 1)
#使用tf.interactiveSession创建默认的session，接着初始化全部参数模型
sess  = tf.InteractiveSession()
tf.global_variables_initializer().run()

for step in range(max_steps):
  start_time = time.time()
  image_batch,label_batch = sess.run([images_train,labels_train])
  _, loss_value = sess.run([train_op,loss],feed_dict={image_holder:image_batch,label_holder:label_batch})
  duration = time.time()-start_time

if step%10 == 0:
  examples_per_sec = batch_size/duration
  sec_per_batch = float(duration)
  
