1.TensorFlow基本操作
# 创建一个变量
w = tf.Variable([[0.5,1.0]])
x = tf.Variable([[2.0],[1.0]]) 
# 创建一个操作
y = tf.matmul(w, x)  
#全局变量初始化
init_op = tf.global_variables_initializer()
with tf.Session() as sess:
  sess.run(init_op)
  print (y.eval())
#生成的值服从具有指定平均值和标准偏差的正态分布
norm = tf.random_normal([2, 3], mean=-1, stddev=4)

# 洗牌
c = tf.constant([[1, 2], [3, 4], [5, 6]])
shuff = tf.random_shuffle(c)

# 每一次执行结果都会不同
sess = tf.Session()
print (sess.run(norm))
print (sess.run(shuff))

2.tensorflow实现回归任务
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
# 随机生成1000个点，围绕在y=0.1x+0.3的直线周围
num_points = 1000
vectors_set = []
for i in range(num_points):
  x1 = np.random.normal(0.0, 0.55)
  y1 = x1 * 0.1 + 0.3 + np.random.normal(0.0, 0.03)
  vectors_set.append([x1, y1])
# 生成一些样本
x_data = [v[0] for v in vectors_set]
y_data = [v[1] for v in vectors_set]
plt.scatter(x_data,y_data,c='orange',s=5)
plt.show()
在定义模型结构之前，先考虑第一个问题，x作为输入数据是几维的？如果只看上图，可能很多读者会认为该数据集是二维的，
但此时关注的仅仅是数据x，y表示标签而非数据，所以模型输入的数据是一维的，这点非常重要，因为要根据数据的维度来设计权重参数。
# 生成1维的W矩阵，取值是[-1,1]之间的随机数
W = tf.Variable(tf.random_uniform([1], -1.0, 1.0), name='W')
# 生成1维的b矩阵，初始值是0
b = tf.Variable(tf.zeros([1]), name='b')
首先要从最终求解的目标下手，回归任务就是要求出其中的权重参数w和偏置参数b。既然数据是一维的，权重参数w必然也是一维的，它们需要一一对应起来。
先对其进行初始化操作，tf.random_uniform([1],−1.0,1.0)表示随机初始化一个数，这里的[1]表示矩阵的维度，如果要创建一个3行4列的矩阵参数就是[3,4]。−1.0和1.0分别表示随机数值的取值范围，
这样就完成权重参数w的初始化工作。偏置参数b的初始化方法类似，但是通常认为偏置对结果的影响较低，以常数0进行初始化即可。
# 经过计算得出预估值y
y = W * x_data + b
# 以预估值y和实际值y_data之间的均方误差作为损失
loss = tf.reduce_mean(tf.square(y - y_data), name='loss')
# 采用梯度下降法来优化参数
optimizer = tf.train.GradientDescentOptimizer(0.5)
目标函数确定之后，接下来就要进行优化，选择梯度下降优化器—tf.train.GradientDescentOptimizer(0.5)，这里传入的0.5表示学习率。在TensorFlow中，优化方法不只有梯度下降优化器，
还有Adam可以自适应调整学习率等策略，需要根据不同任务需求进行选择。
# 训练的过程就是最小化这个误差值
train = optimizer.minimize(loss, name='train')
　接下来要做的就是让优化器朝着损失最小的目标去迭代更新参数，到这里就完成了回归任务中所有要执行的操作。
 sess = tf.Session()
 init = tf.global_variables_initializer()
 sess.run(init)
 # 初始化的W和b是多少
 print ("W =", sess.run(W), "b =", sess.run(b), "loss =", sess.run(loss))
 # 执行20次训练
 for step in range(20):
   sess.run(train)
 # 输出训练好的W和b
   print ("W =", sess.run(W), "b =", sess.run(b), "loss =", sess.run(loss))
 #writer = tf.train.SummaryWriter("./tmp", sess.graph)
