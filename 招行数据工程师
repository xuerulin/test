1.关于数据完整性的说法正确的是 B
 A.实体完整性要求每个实体都必须有一个主键或其他的唯一标识列 /实体完整性要求每个数据表都必须有主键，主键的所有字段属性必须是唯一且非空的；
 B.外键是用来维护两个表之间的级联关系
 C.利用主键约束的列不能有重复的值，但允许NULL值 /不允许有空值
 D.记录中某个字段值为NULL，表示该列上没有值 /NULL是表示这个字段没有填，不是所有字段都允许空值。
2.Python如何定义一个函数
  def&lt;name&gt;(arg1,arg2,...argN)
3.桶排序和快速排序在最好的情况下的时间复杂度分别为O(n),O(nlgn)
4.数据库的封锁机制
  事务包含四个特性（ACID）：原子性（ Atomicity ）、一致性（ Consistency ）、隔离性（ Isolation ）和持续性（ Durability ）
  1、原子性：事务是数据库的逻辑工作单位，事务中包含的各操作要么都做，要么都不做 。
  2、一致性：事务执行的结果必须是使数据库从一个一致性状态变到另一个一致性状态。
  3、隔离性：一个事务的执行不能被其它事务干扰。
  4、持续性：也称永久性，指一个事务一旦提交，它对数据库中的数据的改变就应该是永久性的。
  并发控制：并发操作有可能带来数据不一致性，数据不一致性包括：丢失修改、不可重复读和读“脏”数据
  1.丢失修改：两个事务T1和T2同时读入同一数据并修改，T2提交的结果破坏了T1提交的结果，导致T1的修改被丢失。
  2.不可重复读：由于T2的干扰，导致T1前后两次读的数据不一致。
  3.读‘脏’数据：事务T1修改某一数据并将其写回磁盘，事务T2读取同一数据后，T1由于某种原因被撤销，这时被T1修改过的数据恢复原值，T2读到的数据就与数据库中的数据不一致，
  则T2读到的数据就为“脏”数据，即不正确的数据。（不可重复读和读‘脏’数据要分清，两者不同）
  并发控制用正确的方式调度并发操作，避免上述问题的产生。常用的并发控制技术有封锁、时间戳、乐观控制法和多版本并发控制等
  封锁技术：
  封锁包含四种：共享锁（S锁）、排他锁（X锁）、更新锁（U锁）、意向锁
  共享锁（S锁） 读锁  ：事务T对数据对象A加上S锁，则只允许T读A但不能修改A，其他事务只能再对A加S锁而不能加X锁，直到T释放A上的S锁为止。
  排他锁（X锁） 写锁  ：事务T对数据对象A加上X锁，则只允许T读取和修改A，其他任何事务都不能在对A加任何类型的锁，直到T释放A上的锁为止。
  更新锁（U锁）：更新锁在修改操作的初始化阶段用来锁定可能要被修改的资源，这样可以避免使用共享锁造成的死锁现象。因为使用共享锁时，修改数据的操作分为两步，首先获得一个共享锁，读取数据，
               然后将共享锁升级为排它锁，然后再执行修改操作。这样如果同时有两个或多个事务同时对一个事务申请了共享锁，在修改数据的时候，这些事务都要将共享锁升级为排它锁。
               这时，这些事务都不会释放共享锁而是一直等待对方释放，这样就造成了死锁。如果一个数据在修改前直接申请更新锁，在数据修改的时候再升级为排它锁，就可以避免死锁。
  意向锁：对多粒度树中的结点加意向锁，则说明该结点的下层结点正在被加锁；对任一结点加锁时，必须先对它的上层结点加意向锁。
  封锁协议（加锁解锁的一些规则）
  封锁协议分为一级封锁协议、二级封锁协议、三级封锁协议。
  一级封锁协议：事务T在修改数据A之前必须先对其加X锁，直到事务结束才释放。一级封锁协议可以防止丢失修改，并保证事务T是可恢复的。
  二级封锁协议：二级封锁协议是指，在一级封锁协议基础上增加事务T在读数据A之前必须先对其加S锁，读完后即可释放S锁。二级封锁协议出防止了丢失修改，还可以进一步防止读“脏”数据。
  三级封锁协议：三级封锁协议是指，在二级封锁协议的基础上增加事务T在读数据A之前必须先对其加S锁，直到事务结束才释放。三级封锁协议出防止了丢失修改和读“脏”数据外，还可以进一步防止了不可重复读。
5.ETL过程中的主要环节是抽取（extract）、清洗（clean）、一致性处理（comform）和交付（delivery），简称为ECCD。
6.为什么hive会有多种存储格式？因为hive是文本批处理系统，所以就存在一个往hive中导入数据的问题，首先数据的存储格式有多种，比如数据源是二进制格式， 普通文本格式等等，
  而hive强大之处不要求数据转换成特定的格式，而是利用hadoop本身InputFormat API来从不同的数据源读取数据，同样地使用OutputFormat API将数据写成不同的格式。所以对于不同的数据源，
  或者写出不同的格式就需要不同的对应的InputFormat和Outputformat类的实现。
  textfile：
  文件存储编码格式：文件存储就是正常的文本格式，将表中的数据在hdfs上 以文本的格式存储，下载后可以直接查看，也可以使用cat命令查看
  建表时如何指定： 1.无需指定，默认就是
                 2.显示指定stored as textfile
                 3.显示指定 STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
  优点弊端：1.行存储使用textfile存储文件默认每一行就是一条记录；
           2.可以使用任意的分隔符进行分割。
           3.但无压缩，所以造成存储空间大。可结合Gzip、Bzip2、Snappy等使用（系统自动检查，执行查询时自动解压），但使用这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作。
           
  sequencefile：
  文件存储编码格式：在hdfs上将表中的数据以二进制格式编码，并且将数据压缩了，下载数据以后是二进制格式，不可以直接查看，无法可视化。
  建表时如何指定：1.stored as sequecefile
                2.或者显示指定：STORED AS INPUTFORMAT 
                              'org.apache.hadoop.mapred.SequenceFileInputFormat' 
                              OUTPUTFORMAT 
                              'org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat'
  优点弊端：1.sequencefile存储格有压缩，存储空间小，有利于优化磁盘和I/O性能
           2.同时支持文件切割分片，提供了三种压缩方式：none,record,block（块级别压缩效率跟高）.默认是record(记录)
           3.基于行存储
  
  rcfile：
  文件存储编码格式：在hdfs上将表中的数据以二进制格式编码，并且支持压缩。下载后的数据不可以直接可视化。
  建表时如何指定：1.stored as rcfile 
                 2.或者显示指定：STORED AS INPUTFORMAT 
                               'org.apache.hadoop.hive.ql.io.RCFileInputFormat' 
                               OUTPUTFORMAT 
                               'org.apache.hadoop.hive.ql.io.RCFileOutputFormat'
  优点弊端：1.行列混合的存储格式，基于列存储。
           2.因为基于列存储，列值重复多，所以压缩效率高。
           3.磁盘存储空间小，io小。
7.开窗函数
  开窗函数也叫分析函数，有两类：一类是聚合开窗函数，一类是排序开窗函数。
  我们知道聚合函数对一组值执行计算并返回单一的值，如sum()，count()，max()，min()， avg()等，这些函数常与group by子句连用。除了 COUNT 以外，聚合函数忽略空值。
  开窗函数和聚合函数的区别如下：
 （1）SQL 标准允许将所有聚合函数用作开窗函数，用OVER 关键字区分开窗函数和聚合函数。
 （2）聚合函数每组只返回一个值，开窗函数每组可返回多个值。
8.Python数据类型中，不可以通过索引访问的是集合和字典
9.谈谈数据集市、数据仓库的区别和联系
  （1）数据仓库是企业级的，能为整个企业各个部门的运行提供决策支持手段；
       数据集市则是一种微型的数据仓库,它通常有更少的数据,更少的主题区域,以及更少的历史数据,因此是部门级的，一般只能为某个局部范围内的管理人员服务，因此也称之为部门级数据仓库。
  （2）数据仓库向各个数据集市提供数据
  （3）几个部门的数据集市组成一个数据仓库
  （4） 通常仓库中数据粒度比集市的粒度要细
   作为一个包含多个业务部门，且有大量应用系统的企业，应该如何去设计搭建数据集市。
10.现在需要统计出每个班级中排名前三的学生姓名以及其分数，请写出SQL实现
   select std_nm,score from
   (select std_nm,score row_number() over(partition by class order by score desc)
   as n from pub_f_class_score) where n<=3
11.你对ETL过程中数据清洗的认识
   数据清洗包括以下内容：
   数据补缺：对空数据、缺失数据进行数据补缺操作，无法处理的做标记。
   数据替换：对无效数据进行数据的替换。
   格式规范化：将源数据抽取的数据格式转换成为便于进入仓库处理的目标数据格式。
   主外键约束：通过建立主外键约束，对非法数据进行数据替换或导出到错误文件重新处理。
12.小赵在测试spark的时候，写了如下的代码
do
lines = sc.textFile("data.txt")
pairs = lines.map(lambda s: (s, 1)
counts = pairs.groupByKey(lambda a, b: a + b)
结果运行时等待了一段时间直接报错，data.txt文件较大，小赵对其进行抽样后结果如下：
data
apple
apple
apple
new
name
apple
apple
work
as
请分析报错的原因以及解决方案
参考答案：
报错是由于数据倾斜导致的
数据倾斜的原因，由于key本身分布不均衡（重要），或者shuffle时的并发度不够，过多的数据在同一个task中运行，把executor撑爆。
解决方案（前三点任一）
1）隔离执行，将异常的key过滤出来单独处理，最后与正常数据的处理结果进行union操作。
2）对key先添加随机值，进行操作后，去掉随机值，再进行一次操作：
           将原始的 key 转化为 key + 随机值(例如Random.nextInt)
           对数据进行 reduceByKey(func)
           将 key + 随机值 转成 key
           再对数据进行 reduceByKey(func)
3）使用reduceByKey 代替 groupByKey，reduceByKey已经做过一次merge，节约了内存
4）调高shuffle的并行度
